diff -ur sync/mutex.go /usr/local/Cellar/go/1.13.4/libexec/src/sync/mutex.go
--- sync/mutex.go	2020-01-10 15:18:35.000000000 -0600
+++ /usr/local/Cellar/go/1.13.4/libexec/src/sync/mutex.go	2020-02-18 23:26:44.000000000 -0600
@@ -70,10 +70,15 @@
 // If the lock is already in use, the calling goroutine
 // blocks until the mutex is available.
 func (m *Mutex) Lock() {
+	// Ignore sync operations for race detection
+	if race.Enabled {
+		race.Disable()
+	}
 	// Fast path: grab unlocked mutex.
 	if atomic.CompareAndSwapInt32(&m.state, 0, mutexLocked) {
 		if race.Enabled {
-			race.Acquire(unsafe.Pointer(m))
+			race.Enable()
+			race.PostLock(unsafe.Pointer(m), true)
 		}
 		return
 	}
@@ -166,7 +171,8 @@
 	}
 
 	if race.Enabled {
-		race.Acquire(unsafe.Pointer(m))
+		race.Enable()
+		race.PostLock(unsafe.Pointer(m), true)
 	}
 }
 
@@ -179,7 +185,8 @@
 func (m *Mutex) Unlock() {
 	if race.Enabled {
 		_ = m.state
-		race.Release(unsafe.Pointer(m))
+		race.PreUnlock(unsafe.Pointer(m), true)
+		race.Disable()
 	}
 
 	// Fast path: drop lock bit.
@@ -189,6 +196,9 @@
 		// To hide unlockSlow during tracing we skip one extra frame when tracing GoUnblock.
 		m.unlockSlow(new)
 	}
+	if race.Enabled {
+		race.Enable()
+	}
 }
 
 func (m *Mutex) unlockSlow(new int32) {
